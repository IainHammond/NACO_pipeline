#!/bin/bash
# Usage: sbatch slurm-openmp-job-script
# Prepared By: Kai Xi,  Apr 2015
#              help@massive.org.au

# NOTE: To activate a SLURM option, remove the whitespace between the '#' and 'SBATCH'

# To give your job a name, replace "MyJob" with an appropriate name
#SBATCH --job-name=NACO-CQTau-postproc


# To set a project account for credit charging, 
#SBATCH --account=pd87

# Request CPU resource for a openmp job, suppose it is a 12-thread job
#SBATCH --ntasks=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=8

# Memory usage (MB)
#SBATCH --mem-per-cpu=24000

# Set your minimum acceptable wall time, format: day-hours:minutes:seconds
#SBATCH --time=0-24:00:00

# To receive an email when job completes or fails
#SBATCH --mail-user=<iain.hammond@monash.edu>
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL

# Set the file for output (stdout)
#SBATCH --output=pipeline_output%j.out

# Set the file for error log (stderr)
#SBATCH --error=pipeline_output_error%j.err

# Use reserved node to run job when a node reservation is made for you already
# SBATCH --reservation=reservation_name

# Command to run a openmp job
# Set OMP_NUM_THREADS to the same value as: --cpus-per-task=2
export OMP_NUM_THREADS=8
export OPENBLAS_NUM_THREADS=8
export MKL_NUM_THREADS=8
source /home/ihammond/miniconda3/etc/profile.d/conda.sh # supposed to fix conda activate error
conda activate VIPenv # VIP environment
ulimit -s unlimited # to fix stack size memory error but it doesnt really seem to work
python run_script.py
